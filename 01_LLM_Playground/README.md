
# Pattern: LLM Client Abstraction

### Intent
Decouple application logic from LLM providers.

### Benefits

**Multi-model routing**
**Easier testing & mocking**
**Centralized observability**
**Safer prompt handling**

### Anti-Pattern

**âŒ Calling LLM APIs directly from business logic**
**âŒ Hardcoding prompts inside functions**

### âš ï¸ Known Limitations (By Design)

**No retry logic**
**No cost computation**
**No output schema validation**
**No error handling abstraction**

These are intentionally deferred to future days to keep each artifact focused.

# Implementation

This is a **clean, modular LLM client** that wraps the OpenAI API with metrics tracking. The codebase follows software engineering best practices with separation of concerns and clear abstractions.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   main.py   â”‚ â† Entry point / Example usage
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLMClient      â”‚ â† Orchestrates requests
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    â”‚  LLMRequest  â”‚ â† Data model
         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    â”‚   OpenAI     â”‚ â† External API
         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Metrics    â”‚ â† Latency tracking
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ How to Run

### Setup (First Time)
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure your API key

# Edit .env and add your OpenAI API key
```

### Run the Main Example
```bash
cd src
python main.py
```



## ğŸ“Š Data Flow

```
User Input
   â†“
LLMRequest (structured data)
   â†“
LLMClient.execute()
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Start Metrics       â”‚
â”‚ Call OpenAI API     â”‚
â”‚ Stop Metrics        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
Response Dict {
    output: str,
    usage: TokenUsage,
    latency_ms: float,
    model: str
}
```

---

## ğŸ” Configuration

The `.env` file should contain:
```bash
OPENAI_API_KEY=sk-...your-key-here...
```
## ğŸ¯ Key Takeaways

âœ… **Well-structured**: Clear separation between models, clients, and utilities  
âœ… **Type-safe**: Uses dataclasses and type hints  
âœ… **Maintainable**: Small, focused modules (10-30 lines each)  
âœ… **Observable**: Tracks latency and token usage  
âœ… **Configurable**: Environment-based configuration  
âœ… **Documented**: Comprehensive README and test suite  

This is a solid foundation for LLM experimentation! ğŸš€
